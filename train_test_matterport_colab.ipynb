{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_test_matterport_colab.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"a4G-CQq3MdDS"},"source":["# Main training and testing notebook\n","\n","This notebook is intended to run on Google Colab and on't work in local"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Uyid-CTfQ8A","executionInfo":{"status":"ok","timestamp":1617192856601,"user_tz":-120,"elapsed":531,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"1af26819-c175-498e-905e-5c4810337770"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9ydwyDclY1b1"},"source":["# Ensure folder of execution\n","import os\n","BASE_DIR = '/content/drive/MyDrive/AIMove/Personal_project/matterport'\n","os.chdir(BASE_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvCjVO2J6J-p","executionInfo":{"status":"ok","timestamp":1617192860869,"user_tz":-120,"elapsed":4776,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"3df220e4-02b5-4d90-d637-2189d65f1e8a"},"source":["# Version sensitive imports\n","%tensorflow_version 1.15.0\n","import tensorflow as tf\n","!pip install keras==2.1.0 --user\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `1.15.0`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow 1.x selected.\n","Collecting keras==2.1.0\n","  Using cached https://files.pythonhosted.org/packages/bf/c2/b0c2ece713e754d1692aa432ad682751cd1ad6abf7500a534558b1fbfbe7/Keras-2.1.0-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.0) (1.19.5)\n","\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.0 which is incompatible.\u001b[0m\n","Installing collected packages: keras\n","Successfully installed keras-2.1.0\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q5Ekdzo1iGEB"},"source":["# ASSERT A GPU COLAB\n","\n","# device_name = tf.test.gpu_device_name()\n","# if device_name != '/device:GPU:0':\n","#   raise SystemError('GPU device not found')\n","# print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PpEtXBdaOHI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617192860875,"user_tz":-120,"elapsed":4762,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"902343f6-f006-4d25-d862-c62abf49f19e"},"source":["# INFO ON GPU\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wed Mar 31 10:14:23 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MFZhODvJMNcv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617192861353,"user_tz":-120,"elapsed":5230,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"c7fd3db1-ddcc-4b12-d33d-afca9626a250"},"source":["# Import libraries and tools from matterport maskrcnn\n","\n","import sys\n","import itertools\n","import math\n","import logging\n","import json\n","import re\n","import random\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.lines as lines\n","from matplotlib.patches import Polygon\n","from PIL import Image, ImageDraw\n","import skimage\n","import datetime\n","import keras\n","import time\n","from matplotlib.patches import Rectangle\n","\n","\n","# Import Mask RCNN\n","sys.path.append(os.path.join(BASE_DIR, \"Mask_RCNN\"))\n","from mrcnn import utils\n","from mrcnn import visualize\n","from mrcnn.visualize import display_images\n","from mrcnn.config import Config\n","import mrcnn.model as modellib\n","from mrcnn.model import log\n","from mrcnn.model import mold_image\n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(BASE_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(BASE_DIR, \"models/mask_rcnn_coco.h5\")\n","\n","# Import coco utilities\n","sys.path.append(os.path.join(BASE_DIR, \"Mask_RCNN/samples/coco\"))\n","import coco\n","COCO_DIR = os.path.join(BASE_DIR, \"Mask_RCNN/samples/coco\")\n","\n","sys.path.append(os.path.join(BASE_DIR, \"coco/PythonAPI\"))\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CLZLAZgIQhvy"},"source":[" # Ensure the version\n","! pip uninstall imgaug\n","! pip install imgaug==0.4.0\n","import imgaug.augmenters as iaa\n","import imgaug\n","print(imgaug.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Nc-kPHgcaBa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617192861564,"user_tz":-120,"elapsed":5431,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"992522e0-a182-4ae3-9275-16a4a7ec55e2"},"source":["# Importation of TV cards dataset\n","# This dataset was badly imported and I had to make some adjustments like dividing by 4 the sizes\n","\n","ANNOTATION_FILE_CARDS_ON_TV = os.path.join(BASE_DIR, \"..\", \"tv_cards_dataset\", \"TV cards\", \"labels_cards_on_TV_COCO.json\")\n","DATASET_IMG_DIR_CARDS_ON_TV = os.path.join(BASE_DIR, \"..\", \"tv_cards_dataset\", \"TV cards\", \"cards_on_TV_resized_renamed\")\n","\n","class CardsOnTVDataset(utils.Dataset):\n","\n","  def extract_mask(self, image_id):\n","    with open(ANNOTATION_FILE_CARDS_ON_TV) as f:\n","      labels = json.load(f)\n","\n","    # masks (stored as annotations in COCO format)\n","    annotations = labels['annotations']\n","    classes = labels['categories']\n","    images = labels['images']\n","    id_coco_file = image_id \n","    image = next((image for image in images if image[\"id\"] == id_coco_file))\n","\n","    # dividing by 4 because the images have been resized after labeling....\n","    image_height = image['height'] // 4\n","    image_width = image['width'] // 4\n","\n","    # list containing all the masks of the image of the given id/index\n","    image_masks = list()\n","\n","    image_annotations = list((annotation for annotation in annotations if annotation[\"image_id\"] == id_coco_file))\n","      \n","    segmentation_tuples = list()\n","    class_ids = list()\n","\n","    # only used for drawing \n","    empty_mask = np.zeros((image_height,image_width))\n","    # actual list of masks\n","    concatenated_mask = np.zeros((image_height,image_width, len(image_annotations)))\n","\n","    # for each mask (annotation) belonging to the image we draw it and stack them in masks\n","    for i, annotation in enumerate(image_annotations):\n","\n","      # orignal list contains a succession of x and y coordinates\n","      # we reshape that to obtain a list of x,y tuples\n","      class_ids.append(annotation['category_id'])\n","      # dividing by 4 because the images have been resized after labeling....\n","      segmentation_tuples = np.reshape(np.array(annotation[\"segmentation\"][0])// 4, (int(np.array(annotation[\"segmentation\"][0]).size/2), 2))\n","      img_to_draw_on = Image.fromarray(empty_mask)\n","      draw = ImageDraw.Draw(img_to_draw_on)\n","      draw.polygon([tuple(p) for p in segmentation_tuples], fill=1)\n","      mask = np.asarray(img_to_draw_on)\n","\n","      concatenated_mask[:, :, i] = mask\n","\n","    return concatenated_mask.astype('bool_'), image_width, image_height, np.asarray(class_ids, dtype=np.int32)\n","\n","  def load_cards_on_TV_dataset(self, is_train):\n","    # Add classes\n","    self.add_class(\"cards_on_TV_dataset\", 1, \"green_card\")\n","    self.add_class(\"cards_on_TV_dataset\", 2, \"gold_card\")\n","\n","    for filename in os.listdir(DATASET_IMG_DIR_CARDS_ON_TV):\n","      \n","      image_id = filename[:-4]\n","\n","      # Test set contains 16 images of green cards, 16 images of yellow cards, and 16 images containing both\n","      # Among each set, we have 4 images of zoomed out cards not cropped, 4 images of zoomed out cards cropped,\n","      # 4 images of zoomed in cards not cropped, 4 images of zoomed in cards cropped\n","      # For images where both the cads are present we have 8 images with uncropped cards, 4 with cropped green cards and 4 with cropped yellow cards\n","\n","      if is_train and int(image_id) in [ 11,  15, 188, 187, 319, 322,  35,  67,  81,  89, 127, 234, 267, 232,  84, 88,\n","                                          3,   7, 257, 275, 222, 194, 115, 100, 230, 112, 142, 259, 226, 223, 157,258,\n","                                        244, 272, 159, 120,   9,   2, 137,  28,  19,  23, 204, 310, 312, 303, 237,190, ]:\n","        continue\n","\n","      if not is_train and int(image_id) not in [ 11,  15, 188, 187, 319, 322,  35,  67,  81,  89, 127, 234, 267, 232,  84,  88,\n","                                                  3,   7, 257, 275, 222, 194, 115, 100, 230, 112, 142, 259, 226, 223, 157, 258,\n","                                                244, 272, 159, 120,   9,   2, 137,  28,  19,  23, 204, 310, 312, 303, 237, 190, ]:\n","        continue\n","        \n","      img_path = os.path.join(DATASET_IMG_DIR_CARDS_ON_TV, filename)\n","\n","\n","      self.add_image('cards_on_TV_dataset', image_id=image_id, path=img_path, annotation=ANNOTATION_FILE_CARDS_ON_TV)\n","\n","\n","\n","  def image_reference(self, image_id):\n","    info = self.image_info[image_id]\n","    return info['path']\n","\n","\n","  def load_mask(self, image_id):\n","    info = self.image_info[image_id]\n","\n","    mask , w, h, class_ids= self.extract_mask(int(info['id']))\n","        \n","    return mask, class_ids\n","\n","  def load_image(self, image_id):\n","    image = skimage.io.imread(self.image_info[image_id]['path'])\n","\n","    return image\n","\n","TV_train_set = CardsOnTVDataset()\n","TV_train_set.load_cards_on_TV_dataset(is_train=True)\n","TV_train_set.prepare()\n","print('Train: %d' % len(TV_train_set.image_ids))\n","\n","TV_test_set = CardsOnTVDataset()\n","TV_test_set.load_cards_on_TV_dataset(is_train=False)\n","TV_test_set.prepare()\n","print('Test: %d' % len(TV_test_set.image_ids))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train: 274\n","Test: 48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yYeSeSP46EOC"},"source":["# This function help check that the masks are aligned with the images\n","def visual_check_dataset(dataset):\n","  image_id = 1\n","  image = dataset.load_image(image_id)\n","  print(image.shape)\n","  mask, class_ids = dataset.load_mask(image_id)\n","  print(mask.shape) \n","\n","  for j in range(mask.shape[2]):\n","    plt.imshow(mask[:, :, j], cmap='gray', alpha=0.3)\n","\n","\n","  for i in (count+1 for count in range(9)):\n","\n","\n","\n","    plt.subplot(330 + 1 + i-1)\n","\n","    image = dataset.load_image(i)\n","    plt.imshow(image)\n","\n","    mask, _ = dataset.load_mask(i)\n","    for j in range(mask.shape[2]):\n","      plt.imshow(mask[:, :, j], cmap='gray', alpha=0.3)\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skLpfUM4fmGz"},"source":["# visual_check_dataset(TV_train_set)\n","# visual_check_dataset(TV_test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtJLeOpbSm2Z","executionInfo":{"status":"ok","timestamp":1617192861994,"user_tz":-120,"elapsed":5832,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"6c4b7a70-51ff-478b-9bdd-267dfef5d77e"},"source":["# Loading of cards in box dataset\n","ANNOTATION_FILE_CARDS_ON_BOX = os.path.join(BASE_DIR, \"..\", \"tv_cards_dataset\", \"TV cards\", \"labels_cards_in_box_COCO.json\")\n","DATASET_IMG_DIR_CARDS_ON_BOX = os.path.join(BASE_DIR, \"..\", \"tv_cards_dataset\", \"TV cards\", \"cards_in_box_resized_renamed\")\n","\n","import json\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image, ImageDraw\n","class CardsOnBoxDataset(utils.Dataset):\n","\n","  def extract_mask(self, image_id):\n","    with open(ANNOTATION_FILE_CARDS_ON_BOX) as f:\n","      labels = json.load(f)\n","    # masks (stored as annotations in COCO format)\n","    annotations = labels['annotations']\n","    classes = labels['categories']\n","    images = labels['images']\n","    id_coco_file = image_id\n","    image = next((image for image in images if image[\"id\"] == id_coco_file))\n","\n","    image_height = image['height']\n","    image_width = image['width']\n","\n","    # list containing all the masks of the image of the given id/index\n","    image_masks = list()\n","\n","    image_annotations = list((annotation for annotation in annotations if annotation[\"image_id\"] == id_coco_file))\n","      \n","    segmentation_tuples = list()\n","    class_ids = list()\n","\n","    # only used for drawing \n","    empty_mask = np.zeros((image_height,image_width))\n","    # actual list of masks\n","    concatenated_mask = np.zeros((image_height,image_width, len(image_annotations)))\n","\n","    # for each mask (annotation) belonging to the image we draw it and stack them in masks\n","    for i, annotation in enumerate(image_annotations):\n","\n","      # orignal list contains a succession of x and y coordinates\n","      # we reshape that to obtain a list of x,y tuples\n","      class_ids.append(annotation['category_id'])\n","      segmentation_tuples = np.reshape(np.array(annotation[\"segmentation\"][0]), (int(np.array(annotation[\"segmentation\"][0]).size/2), 2))\n","      img_to_draw_on = Image.fromarray(empty_mask)\n","      draw = ImageDraw.Draw(img_to_draw_on)\n","      draw.polygon([tuple(p) for p in segmentation_tuples], fill=1)\n","      mask = np.asarray(img_to_draw_on)\n","\n","      concatenated_mask[:, :, i] = mask\n","\n","    return concatenated_mask.astype('bool_'), image_width, image_height, np.asarray(class_ids, dtype=np.int32)\n","\n","  def load_cards_on_box_dataset(self, is_train):\n","    # Add classes\n","    self.add_class(\"cards_on_box_dataset\", 1, \"green_card\")\n","    self.add_class(\"cards_on_box_dataset\", 2, \"gold_card\")\n","\n","    for filename in os.listdir(DATASET_IMG_DIR_CARDS_ON_BOX):\n","      \n","      image_id = filename[:-4]\n","\n","      if is_train and int(image_id) in [ 2, 33, 44, 46, 67,\n","                                         1,  5, 10, 78, 17,\n","                                         9, 14, 26, 68, 75 ]:\n","        continue\n","\n","      if not is_train and int(image_id) not in [ 2, 33, 44, 46, 67,\n","                                                 1,  5, 10, 78, 17,\n","                                                 9, 14, 26, 68, 75 ]:\n","        continue\n","        \n","      img_path = os.path.join(DATASET_IMG_DIR_CARDS_ON_BOX, filename)\n","\n","\n","      self.add_image('cards_on_box_dataset', image_id=image_id, path=img_path, annotation=ANNOTATION_FILE_CARDS_ON_BOX)\n","\n","\n","\n","  def image_reference(self, image_id):\n","    info = self.image_info[image_id]\n","    return info['path']\n","\n","\n","  def load_mask(self, image_id):\n","    info = self.image_info[image_id]\n","\n","    mask , w, h, class_ids= self.extract_mask(int(info['id']))\n","        \n","    return mask, class_ids\n","\n","  def load_image(self, image_id):\n","    image = skimage.io.imread(self.image_info[image_id]['path'])\n","\n","    return image\n","\n","box_train_set = CardsOnBoxDataset()\n","box_train_set.load_cards_on_box_dataset(is_train=True)\n","box_train_set.prepare()\n","print('Train: %d' % len(box_train_set.image_ids))\n","\n","box_test_set = CardsOnBoxDataset()\n","box_test_set.load_cards_on_box_dataset(is_train=False)\n","box_test_set.prepare()\n","print('Test: %d' % len(box_test_set.image_ids))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train: 70\n","Test: 15\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uQKMoRsqVaJ5"},"source":["# visual_check_dataset(box_train_set)\n","# visual_check_dataset(box_test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svrw4A-JIAhn","executionInfo":{"status":"ok","timestamp":1617192861997,"user_tz":-120,"elapsed":5818,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"2d51489d-3153-4e9f-d59f-754901f1de5c"},"source":["# Loading of mixed dataset\n","\n","ANNOTATION_FILE_MIXED_CARDS = os.path.join(BASE_DIR, \"..\", \"tv_cards_dataset\", \"TV cards\", \"labels_mixed_cards_COCO.json\")\n","DATASET_IMG_DIR_MIXED_CARDS = os.path.join(BASE_DIR, \"..\", \"tv_cards_dataset\", \"TV cards\", \"mixed_cards_resized_renamed\")\n","\n","class MixedCardsDataset(utils.Dataset):\n","\n","  def extract_mask(self, image_id):\n","    with open(ANNOTATION_FILE_MIXED_CARDS) as f:\n","      labels = json.load(f)\n","    # masks (stored as annotations in COCO format)\n","    annotations = labels['annotations']\n","    classes = labels['categories']\n","    images = labels['images']\n","    id_coco_file = image_id\n","    image = next((image for image in images if image[\"id\"] == id_coco_file))\n","\n","    image_height = image['height'] \n","    image_width = image['width']\n","\n","    # list containing all the masks of the image of the given id/index\n","    image_masks = list()\n","\n","    # image = next((image for image in images if image[\"id\"] == image_id))\n","    image_annotations = list((annotation for annotation in annotations if annotation[\"image_id\"] == id_coco_file))\n","      \n","    segmentation_tuples = list()\n","    class_ids = list()\n","\n","    # only used for drawing \n","    empty_mask = np.zeros((image_height,image_width))\n","    # actual list of masks\n","    concatenated_mask = np.zeros((image_height,image_width, len(image_annotations)))\n","\n","    # for each mask (annotation) belonging to the image we draw it and stack them in masks\n","    for i, annotation in enumerate(image_annotations):\n","\n","      # mask_class = next((mask_class['name'] for mask_class in classes if mask_class[\"id\"] == image_annotations[0]['category_id']))\n","\n","      # orignal list contains a succession of x and y coordinates\n","      # we reshape that to obtain a list of x,y tuples\n","      class_ids.append(annotation['category_id'])\n","      segmentation_tuples = np.reshape(np.array(annotation[\"segmentation\"][0]), (int(np.array(annotation[\"segmentation\"][0]).size/2), 2))\n","      img_to_draw_on = Image.fromarray(empty_mask)\n","      draw = ImageDraw.Draw(img_to_draw_on)\n","      draw.polygon([tuple(p) for p in segmentation_tuples], fill=1)\n","      mask = np.asarray(img_to_draw_on)\n","\n","      concatenated_mask[:, :, i] = mask\n","\n","    return concatenated_mask.astype('bool_'), image_width, image_height, np.asarray(class_ids, dtype=np.int32)\n","\n","  def load_mixed_cards_dataset(self, is_train):\n","    # Add classes\n","    self.add_class(\"cards_on_box_dataset\", 1, \"green_card\")\n","    self.add_class(\"cards_on_box_dataset\", 2, \"gold_card\")\n","\n","    for filename in os.listdir(DATASET_IMG_DIR_MIXED_CARDS):\n","      \n","      image_id = filename[:-4]\n","\n","      if is_train and int(image_id) in [ \n","                                          9,  16,  23,  53,  79,\n","                                        104,  91,  78,  98,  27, \n","                                          2,  44,  24,  76,  71,\n","                                        115, 107, 111,  94, 146, \n","                                        106, 147, 148, 132, 161,\n","                                        122, 138, 137, 154, 168, ]:\n","        continue\n","\n","      if not is_train and int(image_id) not in [ \n","                                          9,  16,  23,  53,  79,\n","                                        104,  91,  78,  98,  27, \n","                                          2,  44,  24,  76,  71,\n","                                        115, 107, 111,  94, 146, \n","                                        106, 147, 148, 132, 161,\n","                                        122, 138, 137, 154, 168, ]:\n","        continue\n","        \n","      img_path = os.path.join(DATASET_IMG_DIR_MIXED_CARDS, filename)\n","\n","\n","      self.add_image('cards_on_box_dataset', image_id=image_id, path=img_path, annotation=ANNOTATION_FILE_MIXED_CARDS)\n","\n","\n","\n","  def image_reference(self, image_id):\n","    info = self.image_info[image_id]\n","    return info['path']\n","\n","\n","  def load_mask(self, image_id):\n","    info = self.image_info[image_id]\n","\n","    mask , w, h, class_ids= self.extract_mask(int(info['id']))\n","        \n","    return mask, class_ids\n","\n","  def load_image(self, image_id):\n","    image = skimage.io.imread(self.image_info[image_id]['path'])\n","\n","    return image\n","\n","mixed_train_set = MixedCardsDataset()\n","mixed_train_set.load_mixed_cards_dataset(is_train=True)\n","mixed_train_set.prepare()\n","print('Train: %d' % len(mixed_train_set.image_ids))\n","\n","mixed_test_set = MixedCardsDataset()\n","mixed_test_set.load_mixed_cards_dataset(is_train=False)\n","mixed_test_set.prepare()\n","print('Test: %d' % len(mixed_test_set.image_ids))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train: 140\n","Test: 30\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p3qoR5fRK4in"},"source":["# visual_check_dataset(mixed_train_set)\n","# visual_check_dataset(mixed_test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFDf1CR-4oJn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617192862000,"user_tz":-120,"elapsed":5800,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"8289cef2-e3f8-4508-eb36-eace00b03483"},"source":["# In order to train or infer, the model need a configuration containing various parameters\n","\n","IMG_RESIZE_DIM = 512\n","DATABASE_SIZE = len(mixed_train_set.image_ids)\n","\n","class TrainConfig(Config):\n","    # Give the configuration a recognizable name\n","    NAME = \"train_config\"\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 2\n","\n","    # Use small images for faster training. Set the limits of the small side\n","    # the large side, and that determines the image shape.\n","    IMAGE_MIN_DIM = IMG_RESIZE_DIM\n","    IMAGE_MAX_DIM = IMG_RESIZE_DIM\n","\n","    # Size of test set. There is 322 images in total.\n","    VALIDATION_STEPS = 322 - DATABASE_SIZE\n","\n","    # Anchor sizes in pixels\n","    RPN_ANCHOR_SCALES = (IMG_RESIZE_DIM/16, IMG_RESIZE_DIM/8, IMG_RESIZE_DIM/4, IMG_RESIZE_DIM/2)\n","\n","    # Aim to allow ROI sampling to pick 33% positive ROIs.\n","    TRAIN_ROIS_PER_IMAGE = 32\n","\n","    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n","    # GPU because the images are small. Batch size is GPUs * images/GPU.\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 2\n","\n","    # DATABASE_SIZE * data augmentation\n","    STEPS_PER_EPOCH = DATABASE_SIZE*8\n","\n","    # F2 normalization\n","    WEIGHT_DECAY = 0.01\n","\n","    LEARNING_RATE = 0.0025\n","    \n","training_config = TrainConfig()\n","training_config.display()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     2\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        100\n","DETECTION_MIN_CONFIDENCE       0.7\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 2\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  512\n","IMAGE_META_SIZE                15\n","IMAGE_MIN_DIM                  512\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [512 512   3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.0025\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               100\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (56, 56)\n","NAME                           train_config\n","NUM_CLASSES                    3\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        1000\n","POST_NMS_ROIS_TRAINING         2000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (32.0, 64.0, 128.0, 256.0)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    256\n","STEPS_PER_EPOCH                1120\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           32\n","USE_MINI_MASK                  True\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               182\n","WEIGHT_DECAY                   0.01\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3qTLsQXb6_F0"},"source":["class InferenceConfig(Config):\n","  NAME = \"inference_config\"\n","  GPU_COUNT = 1\n","  IMAGES_PER_GPU = 1\n","  NUM_CLASSES = 1 + 2\n","  IMAGE_MIN_DIM = IMG_RESIZE_DIM\n","  IMAGE_MAX_DIM = IMG_RESIZE_DIM\n","\n","inference_config = InferenceConfig()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":603},"id":"q-hizHmT0EoT","executionInfo":{"status":"ok","timestamp":1617192868423,"user_tz":-120,"elapsed":12205,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"3dff3148-2806-47d4-84d6-0c87592c42a0"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling imgaug-0.4.0:\n","  Would remove:\n","    /usr/local/lib/python3.7/dist-packages/imgaug-0.4.0.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/imgaug/*\n","Proceed (y/n)? y\n","  Successfully uninstalled imgaug-0.4.0\n","Collecting imgaug==0.4.0\n","  Using cached https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (3.2.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (4.1.2.30)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (7.1.2)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (0.16.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (2.4.1)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.19.5)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug==0.4.0) (1.7.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug==0.4.0) (2.4.7)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug==0.4.0) (2.5)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug==0.4.0) (1.1.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug==0.4.0) (4.4.2)\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.4.0 which is incompatible.\u001b[0m\n","Installing collected packages: imgaug\n","Successfully installed imgaug-0.4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["imgaug"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"7uWWC3LpN_08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617192868426,"user_tz":-120,"elapsed":12200,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"5e313224-deec-454a-e340-454efbe426b9"},"source":["# Augmentation using imgaug. The probability of augmentation has to match the data augmentation\n","# Exemple: if we mulitply the size of the dataset by 4, then data augmentation should happen 75% (0.75) of the time\n","augmentation = iaa.Sometimes(0.815,\n","  iaa.Sequential([\n","    # image deformations and warps, contrast and saturation jittering I want to happen all the time\n","    # Sometimes has a bug, probabilities are inverted on this particular instanciation\n","    iaa.Sometimes(0.5,\n","      iaa.SomeOf(1, None),\n","      [\n","        iaa.OneOf([\n","          iaa.CropAndPad(percent=(-0.3, 0.3)),\n","          iaa.Affine(translate_percent=(-0.25, 0.25)),\n","        ]),\n","        iaa.Affine(rotate=(-90, 90)),\n","        iaa.Affine(shear=(-17, 17)),\n","        iaa.Fliplr(1.0),\n","        iaa.Flipud(1.0),\n","        iaa.OneOf([\n","          iaa.LinearContrast((0.5, 2.0)),\n","          iaa.Multiply((0.3, 2.5)),         \n","        ]),\n","      ]\n","    ),\n","    # color saturation changes should happen 75% of the time\n","    iaa.Sometimes(0.75,\n","      iaa.OneOf([\n","        # 2/3 chances to decrease saturation\n","        iaa.MultiplySaturation((0.1, 0.6)),\n","        iaa.MultiplySaturation((0.1, 0.6)),\n","        iaa.MultiplySaturation((1.5, 5))      \n","      ])\n","    ),\n","    # bluring should happen 50% of the time\n","    iaa.Sometimes(0.5,\n","      iaa.OneOf([\n","        iaa.GaussianBlur((0.3, 3.5)),\n","        iaa.AverageBlur(k=(2, 8)),\n","        iaa.MedianBlur(k=(3, 13)),\n","      ]),\n","    )\n","  ])\n",")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MGHWzjWX-DBB"},"source":["# This code allows to visualize data augmentation\n","\n","# import random as rand\n","\n","# for i in range(30):\n","#   id = rand.randrange(0, 274)\n","\n","#   image = TV_train_set.load_image(id)\n","\n","#   image_aug = augmentation.augment_image(image=image)\n","\n","#   # plt.subplot(1, 5, i+1, figsize=(15,15))\n","#   plt.imshow(image_aug)\n","#   plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duncFMy5DjGm"},"source":["# CUSTOM CALLBACK TO LOG LOSS AND VAL_LOSS BECAUSE TENSORBOARD DOES NOT WORK WITH TF 1.13\n","\n","class LossStoringCallback(keras.callbacks.Callback):\n","\n","  def on_train_begin(self, logs=None):\n","    f_loss = open(os.path.join(BASE_DIR, \"logs\", \"loss.txt\" ), \"w\")\n","    f_loss.write(\"[ \")\n","    f_loss.close()\n","    f_val_loss = open(os.path.join(BASE_DIR, \"logs\", \"val_loss.txt\" ), \"w\")\n","    f_val_loss.write(\"[ \")\n","    f_val_loss.close()\n","\n","  def on_epoch_end(self, epoch, logs=None):\n","    f_loss = open(os.path.join(BASE_DIR, \"logs\", \"loss.txt\" ), \"a\")\n","    f_loss.write(f'{round(logs[\"loss\"], 3)}, ')\n","    f_loss.close()\n","    f_val_loss = open(os.path.join(BASE_DIR, \"logs\", \"val_loss.txt\" ), \"a\")\n","    f_val_loss.write(f'{round(logs[\"val_loss\"], 3)}, ')\n","    f_val_loss.close()\n","\n","  def on_train_end(self, logs=None):\n","    f_loss = open(os.path.join(BASE_DIR, \"logs\", \"loss.txt\" ), \"a\")\n","    f_loss.write(\"]\")\n","    f_loss.close()\n","    f_val_loss = open(os.path.join(BASE_DIR, \"logs\", \"val_loss.txt\" ), \"a\")\n","    f_val_loss.write(\"]\")\n","    f_val_loss.close()\n","\n","loss_storing_callback = LossStoringCallback()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d40WUx7tz5K9"},"source":["early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n","  monitor='val_loss', min_delta=0, patience=7, verbose=0,\n","  mode='auto', baseline=None, restore_best_weights=True\n",")\n","\n","reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n","  factor=0.5, patience=3, min_lr=0\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWNze4IKSOzz"},"source":["# This learning rate scheduler allows starting with a high learning rate by increasing it after the first epochs\n","# If we satrt from the begining with a high learning rate, the loss funxtions might be unstable and not work\n","def scheduler(epoch, lr):\n","  if epoch == 1:\n","    return lr*2\n","  else:\n","    return lr\n","\n","lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQcEfHMV5zhW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1617200456239,"user_tz":-120,"elapsed":7599991,"user":{"displayName":"Etienne Platini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidA7Al0I761OB2uBM9f-Ti557lTxpeQtcrJIDs3A=s64","userId":"14085974224537609750"}},"outputId":"27105775-0b7d-4f88-bc18-8eacab91e93b"},"source":["# Train\n","train_model = modellib.MaskRCNN(mode='training', model_dir=os.path.join(BASE_DIR, 'models', 'transfer_learning'), config=training_config)\n","train_model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n","train_model.train(mixed_train_set, mixed_test_set, learning_rate=training_config.LEARNING_RATE, epochs=1000, layers=\"heads\", augmentation=augmentation, custom_callbacks=[lr_scheduler_callback, early_stopping_callback, reduce_lr_callback, loss_storing_callback])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","\n","Starting at epoch 0. LR=0.0025\n","\n","Checkpoint Path: /content/drive/MyDrive/AIMove/Personal_project/matterport/models/transfer_learning/train_config20210331T1014/mask_rcnn_train_config_{epoch:04d}.h5\n","Selecting layers to train\n","fpn_c5p5               (Conv2D)\n","fpn_c4p4               (Conv2D)\n","fpn_c3p3               (Conv2D)\n","fpn_c2p2               (Conv2D)\n","fpn_p5                 (Conv2D)\n","fpn_p2                 (Conv2D)\n","fpn_p3                 (Conv2D)\n","fpn_p4                 (Conv2D)\n","In model:  rpn_model\n","    rpn_conv_shared        (Conv2D)\n","    rpn_class_raw          (Conv2D)\n","    rpn_bbox_pred          (Conv2D)\n","mrcnn_mask_conv1       (TimeDistributed)\n","mrcnn_mask_bn1         (TimeDistributed)\n","mrcnn_mask_conv2       (TimeDistributed)\n","mrcnn_mask_bn2         (TimeDistributed)\n","mrcnn_class_conv1      (TimeDistributed)\n","mrcnn_class_bn1        (TimeDistributed)\n","mrcnn_mask_conv3       (TimeDistributed)\n","mrcnn_mask_bn3         (TimeDistributed)\n","mrcnn_class_conv2      (TimeDistributed)\n","mrcnn_class_bn2        (TimeDistributed)\n","mrcnn_mask_conv4       (TimeDistributed)\n","mrcnn_mask_bn4         (TimeDistributed)\n","mrcnn_bbox_fc          (TimeDistributed)\n","mrcnn_mask_deconv      (TimeDistributed)\n","mrcnn_class_logits     (TimeDistributed)\n","mrcnn_mask             (TimeDistributed)\n"],"name":"stdout"},{"output_type":"stream","text":["/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1000\n","1120/1120 [==============================] - 410s 366ms/step - loss: 0.9976 - rpn_class_loss: 0.0171 - rpn_bbox_loss: 0.4057 - mrcnn_class_loss: 0.2027 - mrcnn_bbox_loss: 0.2179 - mrcnn_mask_loss: 0.1541 - val_loss: 1.8190 - val_rpn_class_loss: 0.0155 - val_rpn_bbox_loss: 0.4476 - val_mrcnn_class_loss: 0.1070 - val_mrcnn_bbox_loss: 0.0906 - val_mrcnn_mask_loss: 0.1102\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/1000\n","1120/1120 [==============================] - 365s 326ms/step - loss: 0.9757 - rpn_class_loss: 0.0199 - rpn_bbox_loss: 0.5491 - mrcnn_class_loss: 0.1447 - mrcnn_bbox_loss: 0.1348 - mrcnn_mask_loss: 0.1270 - val_loss: 0.7266 - val_rpn_class_loss: 0.0157 - val_rpn_bbox_loss: 0.5015 - val_mrcnn_class_loss: 0.1481 - val_mrcnn_bbox_loss: 0.0989 - val_mrcnn_mask_loss: 0.1232\n","Epoch 3/1000\n","1120/1120 [==============================] - 384s 342ms/step - loss: 0.9681 - rpn_class_loss: 0.0263 - rpn_bbox_loss: 0.6247 - mrcnn_class_loss: 0.0966 - mrcnn_bbox_loss: 0.1069 - mrcnn_mask_loss: 0.1131 - val_loss: 0.4866 - val_rpn_class_loss: 0.0302 - val_rpn_bbox_loss: 0.6514 - val_mrcnn_class_loss: 0.1362 - val_mrcnn_bbox_loss: 0.1223 - val_mrcnn_mask_loss: 0.1200\n","Epoch 4/1000\n","1120/1120 [==============================] - 383s 342ms/step - loss: 0.7588 - rpn_class_loss: 0.0137 - rpn_bbox_loss: 0.4569 - mrcnn_class_loss: 0.0960 - mrcnn_bbox_loss: 0.0912 - mrcnn_mask_loss: 0.1003 - val_loss: 1.0228 - val_rpn_class_loss: 0.0118 - val_rpn_bbox_loss: 0.5629 - val_mrcnn_class_loss: 0.1558 - val_mrcnn_bbox_loss: 0.0731 - val_mrcnn_mask_loss: 0.0962\n","Epoch 5/1000\n","1120/1120 [==============================] - 385s 343ms/step - loss: 0.8224 - rpn_class_loss: 0.0175 - rpn_bbox_loss: 0.5322 - mrcnn_class_loss: 0.0857 - mrcnn_bbox_loss: 0.0853 - mrcnn_mask_loss: 0.1007 - val_loss: 1.0404 - val_rpn_class_loss: 0.0472 - val_rpn_bbox_loss: 0.5351 - val_mrcnn_class_loss: 0.0767 - val_mrcnn_bbox_loss: 0.0855 - val_mrcnn_mask_loss: 0.1137\n","Epoch 6/1000\n","1120/1120 [==============================] - 383s 342ms/step - loss: 0.7499 - rpn_class_loss: 0.0166 - rpn_bbox_loss: 0.4769 - mrcnn_class_loss: 0.0790 - mrcnn_bbox_loss: 0.0822 - mrcnn_mask_loss: 0.0940 - val_loss: 0.8012 - val_rpn_class_loss: 0.0138 - val_rpn_bbox_loss: 0.4598 - val_mrcnn_class_loss: 0.0851 - val_mrcnn_bbox_loss: 0.0808 - val_mrcnn_mask_loss: 0.1157\n","Epoch 7/1000\n","1120/1120 [==============================] - 392s 350ms/step - loss: 0.4522 - rpn_class_loss: 0.0132 - rpn_bbox_loss: 0.2425 - mrcnn_class_loss: 0.0573 - mrcnn_bbox_loss: 0.0551 - mrcnn_mask_loss: 0.0827 - val_loss: 0.3512 - val_rpn_class_loss: 0.0117 - val_rpn_bbox_loss: 0.2920 - val_mrcnn_class_loss: 0.1069 - val_mrcnn_bbox_loss: 0.0640 - val_mrcnn_mask_loss: 0.1013\n","Epoch 8/1000\n","1120/1120 [==============================] - 393s 351ms/step - loss: 0.4074 - rpn_class_loss: 0.0089 - rpn_bbox_loss: 0.2121 - mrcnn_class_loss: 0.0532 - mrcnn_bbox_loss: 0.0502 - mrcnn_mask_loss: 0.0817 - val_loss: 0.7899 - val_rpn_class_loss: 0.0105 - val_rpn_bbox_loss: 0.3114 - val_mrcnn_class_loss: 0.0633 - val_mrcnn_bbox_loss: 0.0589 - val_mrcnn_mask_loss: 0.0966\n","Epoch 9/1000\n","1120/1120 [==============================] - 391s 349ms/step - loss: 0.3767 - rpn_class_loss: 0.0109 - rpn_bbox_loss: 0.2070 - mrcnn_class_loss: 0.0318 - mrcnn_bbox_loss: 0.0476 - mrcnn_mask_loss: 0.0781 - val_loss: 0.5654 - val_rpn_class_loss: 0.0147 - val_rpn_bbox_loss: 0.2872 - val_mrcnn_class_loss: 0.0339 - val_mrcnn_bbox_loss: 0.0508 - val_mrcnn_mask_loss: 0.0989\n","Epoch 10/1000\n","1120/1120 [==============================] - 393s 351ms/step - loss: 0.3834 - rpn_class_loss: 0.0079 - rpn_bbox_loss: 0.2055 - mrcnn_class_loss: 0.0424 - mrcnn_bbox_loss: 0.0467 - mrcnn_mask_loss: 0.0796 - val_loss: 1.5200 - val_rpn_class_loss: 0.0121 - val_rpn_bbox_loss: 0.2744 - val_mrcnn_class_loss: 0.0583 - val_mrcnn_bbox_loss: 0.0521 - val_mrcnn_mask_loss: 0.0957\n","Epoch 11/1000\n","1120/1120 [==============================] - 392s 350ms/step - loss: 0.3076 - rpn_class_loss: 0.0096 - rpn_bbox_loss: 0.1360 - mrcnn_class_loss: 0.0442 - mrcnn_bbox_loss: 0.0405 - mrcnn_mask_loss: 0.0760 - val_loss: 1.4487 - val_rpn_class_loss: 0.0147 - val_rpn_bbox_loss: 0.2228 - val_mrcnn_class_loss: 0.0531 - val_mrcnn_bbox_loss: 0.0368 - val_mrcnn_mask_loss: 0.0939\n","Epoch 12/1000\n","1120/1120 [==============================] - 397s 355ms/step - loss: 0.2877 - rpn_class_loss: 0.0100 - rpn_bbox_loss: 0.1300 - mrcnn_class_loss: 0.0367 - mrcnn_bbox_loss: 0.0376 - mrcnn_mask_loss: 0.0721 - val_loss: 0.2060 - val_rpn_class_loss: 0.0097 - val_rpn_bbox_loss: 0.1976 - val_mrcnn_class_loss: 0.0455 - val_mrcnn_bbox_loss: 0.0417 - val_mrcnn_mask_loss: 0.1040\n","Epoch 13/1000\n","1120/1120 [==============================] - 394s 352ms/step - loss: 0.2721 - rpn_class_loss: 0.0085 - rpn_bbox_loss: 0.1239 - mrcnn_class_loss: 0.0284 - mrcnn_bbox_loss: 0.0361 - mrcnn_mask_loss: 0.0738 - val_loss: 0.5229 - val_rpn_class_loss: 0.0066 - val_rpn_bbox_loss: 0.1838 - val_mrcnn_class_loss: 0.0297 - val_mrcnn_bbox_loss: 0.0375 - val_mrcnn_mask_loss: 0.0978\n","Epoch 14/1000\n","1120/1120 [==============================] - 397s 355ms/step - loss: 0.2665 - rpn_class_loss: 0.0075 - rpn_bbox_loss: 0.1252 - mrcnn_class_loss: 0.0242 - mrcnn_bbox_loss: 0.0353 - mrcnn_mask_loss: 0.0730 - val_loss: 0.4623 - val_rpn_class_loss: 0.0091 - val_rpn_bbox_loss: 0.1929 - val_mrcnn_class_loss: 0.0289 - val_mrcnn_bbox_loss: 0.0357 - val_mrcnn_mask_loss: 0.0956\n","Epoch 15/1000\n","1120/1120 [==============================] - 399s 356ms/step - loss: 0.2465 - rpn_class_loss: 0.0061 - rpn_bbox_loss: 0.1102 - mrcnn_class_loss: 0.0228 - mrcnn_bbox_loss: 0.0340 - mrcnn_mask_loss: 0.0722 - val_loss: 0.3038 - val_rpn_class_loss: 0.0134 - val_rpn_bbox_loss: 0.2094 - val_mrcnn_class_loss: 0.0448 - val_mrcnn_bbox_loss: 0.0368 - val_mrcnn_mask_loss: 0.0923\n","Epoch 16/1000\n","1120/1120 [==============================] - 397s 355ms/step - loss: 0.2269 - rpn_class_loss: 0.0054 - rpn_bbox_loss: 0.0896 - mrcnn_class_loss: 0.0301 - mrcnn_bbox_loss: 0.0307 - mrcnn_mask_loss: 0.0700 - val_loss: 0.3553 - val_rpn_class_loss: 0.0089 - val_rpn_bbox_loss: 0.1834 - val_mrcnn_class_loss: 0.0315 - val_mrcnn_bbox_loss: 0.0339 - val_mrcnn_mask_loss: 0.0915\n","Epoch 17/1000\n","1120/1120 [==============================] - 397s 354ms/step - loss: 0.2292 - rpn_class_loss: 0.0075 - rpn_bbox_loss: 0.0926 - mrcnn_class_loss: 0.0283 - mrcnn_bbox_loss: 0.0312 - mrcnn_mask_loss: 0.0685 - val_loss: 0.2136 - val_rpn_class_loss: 0.0107 - val_rpn_bbox_loss: 0.1765 - val_mrcnn_class_loss: 0.0470 - val_mrcnn_bbox_loss: 0.0350 - val_mrcnn_mask_loss: 0.0942\n","Epoch 18/1000\n","1120/1120 [==============================] - 394s 352ms/step - loss: 0.2357 - rpn_class_loss: 0.0067 - rpn_bbox_loss: 0.1000 - mrcnn_class_loss: 0.0306 - mrcnn_bbox_loss: 0.0299 - mrcnn_mask_loss: 0.0675 - val_loss: 0.1441 - val_rpn_class_loss: 0.0073 - val_rpn_bbox_loss: 0.1611 - val_mrcnn_class_loss: 0.0335 - val_mrcnn_bbox_loss: 0.0374 - val_mrcnn_mask_loss: 0.0928\n","Epoch 19/1000\n"," 966/1120 [========================>.....] - ETA: 49s - loss: 0.2148 - rpn_class_loss: 0.0042 - rpn_bbox_loss: 0.0824 - mrcnn_class_loss: 0.0286 - mrcnn_bbox_loss: 0.0298 - mrcnn_mask_loss: 0.0688"],"name":"stdout"},{"output_type":"stream","text":["Process ForkPoolWorker-4:\n","Process ForkPoolWorker-1:\n","Process ForkPoolWorker-3:\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Process ForkPoolWorker-2:\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n","    task = get()\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n","    with self._rlock:\n","  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n","    return self._semlock.__enter__()\n","  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n","    task = get()\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n","    res = self._reader.recv_bytes()\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n","    buf = self._recv_bytes(maxlength)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n","    result = (True, func(*args, **kwds))\n","  File \"/tensorflow-1.15.2/python3.7/keras/utils/data_utils.py\", line 650, in next_sample\n","    return six.next(_SHARED_SEQUENCES[uid])\n","  File \"/content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py\", line 1774, in data_generator\n","    batch_images[b] = mold_image(image.astype(np.float32), config)\n","  File \"/content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py\", line 2805, in mold_image\n","    return images.astype(np.float32) - config.MEAN_PIXEL\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n","    self._target(*self._args, **self._kwargs)\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-4ece06839dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transfer_learning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mrcnn_class_logits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_bbox_fc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_bbox\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixed_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"heads\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_scheduler_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_storing_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2374\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2377\u001b[0m         )\n\u001b[1;32m   2378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"stream","text":["  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n","    result = (True, func(*args, **kwds))\n","  File \"/tensorflow-1.15.2/python3.7/keras/utils/data_utils.py\", line 650, in next_sample\n","    return six.next(_SHARED_SEQUENCES[uid])\n","  File \"/content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py\", line 1711, in data_generator\n","    use_mini_mask=config.USE_MINI_MASK)\n","  File \"/content/drive/MyDrive/AIMove/Personal_project/matterport/Mask_RCNN/mrcnn/model.py\", line 1251, in load_image_gt\n","    image = det.augment_image(image)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 771, in augment_image\n","    return self.augment_images([image], hooks=hooks)[0]\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 825, in augment_images\n","    hooks=hooks\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 645, in augment_batch_\n","    hooks=hooks)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 3636, in _augment_batch_\n","    hooks=hooks\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 645, in augment_batch_\n","    hooks=hooks)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 3127, in _augment_batch_\n","    hooks=hooks\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 645, in augment_batch_\n","    hooks=hooks)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 3636, in _augment_batch_\n","    hooks=hooks\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 645, in augment_batch_\n","    hooks=hooks)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 3406, in _augment_batch_\n","    hooks=hooks\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/meta.py\", line 645, in augment_batch_\n","    hooks=hooks)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/color.py\", line 1686, in _augment_batch_\n","    batch.images = self._hs_to_images_(batch.images, images_hsv)\n","  File \"/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/color.py\", line 1728, in _hs_to_images_\n","    (np.mod(hue_aug, 255).astype(np.float32) / 255.0)\n","KeyboardInterrupt\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ZEREeEeqnMxF"},"source":["# Model in inference mode\n","inference_model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=os.path.join(BASE_DIR, 'models', 'transfer_learning'))\n","\n","inference_model.load_weights(os.path.join(BASE_DIR, 'models', \"BOX_DATAAUG_8.h5\"), by_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHz7kF2guLd5"},"source":["# You will find here various visualizations of the inference\n","image = test_set.load_image(0)\n","scaled_image = mold_image(image, inference_config)\n","sample = np.expand_dims(scaled_image, 0)\n","r = inference_model.detect(sample, verbose=0)[0]\n","\n","masked_image = visualize.display_instances(image, r['rois'], r['masks'],\n","                                            r['class_ids'], test_set.class_names,\n","                                            r['scores'])\n","\n","masked_image.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GXUdsN-OzNA"},"source":["# Note: there are glitches in the boxes, making some boxes appear superposed when there is only one box\n","def custom_plot_actual_vs_predicted(dataset, model, cfg, image_id):\n","\n","  image = dataset.load_image(image_id)\n","  mask, _ = dataset.load_mask(image_id)\n","\n","  scaled_image = mold_image(image, cfg)\n","  sample = np.expand_dims(scaled_image, 0)\n","  yhat = model.detect(sample, verbose=0)[0]\n","\n","  plt.imshow(image)\n","  plt.title('Actual')\n","\n","  for j in range(mask.shape[2]):\n","    plt.imshow(mask[:, :, j], cmap='gray', alpha=0.3)\n","\n","  plt.imshow(image)\n","  plt.title(f'Predicted {image_id}')\n","  ax = plt.gca()\n","  for box in yhat['rois']:\n","    y1, x1, y2, x2 = box\n","    width, height = x2 - x1, y2 - y1\n","    rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n","    ax.add_patch(rect)\n","    \n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLvLSfjiPNIV"},"source":["# custom_plot_actual_vs_predicted(test_set, model, inference_config, 11)\n","for i in range(45):\n","  custom_plot_actual_vs_predicted(cards_on_box_dataset, inference_model, inference_config, i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBZq10S_dJoH"},"source":["# Bit of useful code that recover the name of an image, gt and predictions to check if visualizations are correct\n","image_id = 31\n","dataset = test_set\n","config = inference_config\n","model = inference_model\n","\n","image = dataset.load_image(image_id)\n","image_path = dataset.image_reference(image_id)\n","image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","      modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n","info = dataset.image_info[image_id]\n","results = model.detect([image], verbose=0)\n","r = results[0]\n","print(r[\"masks\"].shape)\n","print(image_path)\n","print(gt_bbox)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKf8kEJ4Y2W9"},"source":["def display_full_size_predictions(model, dataset, size_dataset, config):\n","  for image_id in range(size_dataset):\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","      modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n","    info = dataset.image_info[image_id]\n","\n","    # Run object detection\n","    results = model.detect([image], verbose=0)\n","\n","    # Display results\n","    _, ax = plt.subplots(1, 1, figsize=(16, 16))\n","    r = results[0]\n","    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n","                                dataset.class_names, r['scores'], ax=ax,\n","                                title=f\"Predictions {image_id}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFTfEKYGZyCh"},"source":["display_full_size_predictions(inference_model, cards_on_box_dataset, 40, inference_config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7h3Nv23o5O37"},"source":["# Functions that compute mAP from matterport MaskRCNN. Copied to have a close look and add some prints and comments.\n","\n","def compute_overlaps_masks(masks1, masks2):\n","    \"\"\"Computes IoU overlaps between two sets of masks.\n","    masks1, masks2: [Height, Width, instances]\n","    \"\"\"\n","    \n","    # If either set of masks is empty return empty result\n","    if masks1.shape[-1] == 0 or masks2.shape[-1] == 0:\n","        return np.zeros((masks1.shape[-1], masks2.shape[-1]))\n","    # flatten masks and compute their areas\n","    masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)\n","    masks2 = np.reshape(masks2 > .5, (-1, masks2.shape[-1])).astype(np.float32)\n","    area1 = np.sum(masks1, axis=0)\n","    area2 = np.sum(masks2, axis=0)\n","\n","    # intersections and union\n","    intersections = np.dot(masks1.T, masks2)\n","    union = area1[:, None] + area2[None, :] - intersections\n","    overlaps = intersections / union\n","\n","    return overlaps\n","\n","def trim_zeros(x):\n","    \"\"\"It's common to have tensors larger than the available data and\n","    pad with zeros. This function removes rows that are all zeros.\n","\n","    x: [rows, columns].\n","    \"\"\"\n","    assert len(x.shape) == 2\n","    return x[~np.all(x == 0, axis=1)]\n","\n","def compute_matches(gt_boxes, gt_class_ids, gt_masks,\n","                    pred_boxes, pred_class_ids, pred_scores, pred_masks,\n","                    iou_threshold=0.5, score_threshold=0.0):\n","    \"\"\"Finds matches between prediction and ground truth instances.\n","\n","    Returns:\n","        gt_match: 1-D array. For each GT box it has the index of the matched\n","                  predicted box.\n","        pred_match: 1-D array. For each predicted box, it has the index of\n","                    the matched ground truth box.\n","        overlaps: [pred_boxes, gt_boxes] IoU overlaps.\n","    \"\"\"\n","\n","    # IMPORTANT\n","    # The IOU is calculated on masks and not bounding boxes.\n","\n","    # This whole bit is trimming zero padding and asserting there is the right number of masks and boxes \n","    # Useless most of the time in our case\n","    gt_boxes = trim_zeros(gt_boxes)\n","    gt_masks = gt_masks[..., :gt_boxes.shape[0]]\n","    pred_boxes = trim_zeros(pred_boxes)\n","    pred_scores = pred_scores[:pred_boxes.shape[0]]\n","    \n","    # Sort predictions by score from high to low\n","\n","    # Makes an array in which are the position of each indices after sorting\n","    indices = np.argsort(pred_scores)[::-1]\n","    # Sort the arrays\n","    pred_boxes = pred_boxes[indices]\n","    pred_class_ids = pred_class_ids[indices]\n","    pred_scores = pred_scores[indices]\n","    pred_masks = pred_masks[..., indices]\n","\n","    # Compute IoU overlaps for masks\n","    # Overlaps is a confusion matrix with the two mask sets\n","    overlaps = compute_overlaps_masks(pred_masks, gt_masks)\n","    \n","    match_count = 0\n","    # Put one -1 on the array by prediction\n","    pred_match = -1 * np.ones([pred_boxes.shape[0]])\n","    gt_match = -1 * np.ones([gt_boxes.shape[0]])\n","\n","    # Loop through predicted boxes\n","    for i in range(len(pred_boxes)):\n","        # Find best matching ground truth box\n","        # 1. Sort mask matches by score\n","        sorted_ixs = np.argsort(overlaps[i])[::-1]\n","        # 2. Remove scores so low we don't even look at them\n","        low_score_idx = np.where(overlaps[i, sorted_ixs] < score_threshold)[0]\n","        if low_score_idx.size > 0:\n","            sorted_ixs = sorted_ixs[:low_score_idx[0]]\n","        # 3. Find the match\n","        for j in sorted_ixs:\n","            # If current iterating ground truth box is already matched, go to next one\n","            if gt_match[j] > -1:\n","                continue\n","            # If we reach IoU smaller than the threshold, end the loop\n","            iou = overlaps[i, j]\n","            if iou < iou_threshold:\n","                break\n","            # If classes match\n","            if pred_class_ids[i] == gt_class_ids[j]:\n","                match_count += 1\n","                gt_match[j] = i\n","                pred_match[i] = j\n","                break\n","\n","    return gt_match, pred_match, overlaps\n","\n","\n","def custom_compute_ap(gt_boxes, gt_class_ids, gt_masks,\n","               pred_boxes, pred_class_ids, pred_scores, pred_masks,\n","               iou_threshold=0.5):\n","    \"\"\"Compute Average Precision at a set IoU threshold (default 0.5).\n","\n","    Returns:\n","    mAP: Mean Average Precision\n","    precisions: List of precisions at different class score thresholds.\n","    recalls: List of recall values at different class score thresholds.\n","    overlaps: [pred_boxes, gt_boxes] IoU overlaps.\n","    \"\"\"\n","    # Get matches and overlaps\n","    gt_match, pred_match, overlaps = compute_matches(\n","        gt_boxes, gt_class_ids, gt_masks,\n","        pred_boxes, pred_class_ids, pred_scores, pred_masks,\n","        iou_threshold)\n","\n","    # Compute precision and recall at each prediction box step\n","    precisions = np.cumsum(pred_match > -1) / (np.arange(len(pred_match)) + 1)\n","    recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)\n","\n","    # Pad with start and end values to simplify the math\n","    precisions = np.concatenate([[0], precisions, [0]])\n","    recalls = np.concatenate([[0], recalls, [1]])\n","\n","    # Ensure precision values decrease but don't increase. This way, the\n","    # precision value at each recall threshold is the maximum it can be\n","    # for all following recall thresholds, as specified by the VOC paper.\n","    for i in range(len(precisions) - 2, -1, -1):\n","        precisions[i] = np.maximum(precisions[i], precisions[i + 1])\n","\n","    # Compute mean AP over recall range\n","    indices = np.where(recalls[:-1] != recalls[1:])[0] + 1\n","    mAP = np.sum((recalls[indices] - recalls[indices - 1]) *\n","                 precisions[indices])\n","\n","    return mAP, precisions, recalls, overlaps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3ET1Oxn5XcC"},"source":["def compute_mAP(model, dataset, nb_images_to_test_on, inference_config, IoU_threshold = 0.5 ):\n","  # Compute VOC-Style mAP @ IoU=0.5\n","  APs = []\n","\n","  # modellib.load_image\n","  # image: np array of the image\n","  # image_meta:\n","  # gt_class_id: class_ids contained in image\n","  # gt_box: coordinates of boxes in pixels\n","  # gt_mask: one hot map of masks, with one channel for each class\n","\n","  for image_id in range(nb_images_to_test_on):\n","      # Load image and ground truth data\n","      image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","          modellib.load_image_gt(dataset, inference_config,\n","                                image_id, use_mini_mask=False)\n","      \n","      # Type of normalization by substracting the mean pixel\n","      molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n","      # Run object detection\n","      results = model.detect([image], verbose=0)\n","      r = results[0]\n","      # Compute AP\n","      AP, precisions, recalls, overlaps =\\\n","          custom_compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                          r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold = IoU_threshold)\n","      APs.append(AP)\n","      \n","  return np.mean(APs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kYwpCqTjGKER","executionInfo":{"status":"ok","timestamp":1617147268355,"user_tz":-120,"elapsed":139847,"user":{"displayName":"Etienne Platini","photoUrl":"","userId":"01355754336263421503"}},"outputId":"3f03ca1e-362e-407c-c1d9-d39dad8c899c"},"source":["# Compute primary challenge metric of COCO: mAP@.5:0.05:0.95\n","mAPs = []\n","\n","# range() only works with ints...\n","for IoU in range(50, 100, 5):\n","  IoU /= 100\n","  mAPs.append(compute_mAP(inference_model, TV_test_set, 48, inference_config, IoU_threshold = IoU ))\n","  print(f'IoU {IoU} OK')\n","\n","print(f'COCO primary challenge metric: {round(np.mean(mAPs), 4)}')\n","print(f'PASCAL VOC 2012 metric: {round(mAPs[0], 4)}')\n","print(f'COCO strict metric: {round(mAPs[5], 4)}')\n","print(f'  0.50    0.55    0.60    0.65    0.70    0.75    0.80    0.85    0.90    0.95')\n","print(f'[ {round(mAPs[0], 4)}, {round(mAPs[1], 4)}, {round(mAPs[2], 4)}, {round(mAPs[3], 4)}, {round(mAPs[4], 4)}, {round(mAPs[5], 4)}, {round(mAPs[6], 4)}, {round(mAPs[7], 4)}, {round(mAPs[8], 4)}, {round(mAPs[9], 4)} ]')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","IoU 0.5 OK\n","IoU 0.55 OK\n","IoU 0.6 OK\n","IoU 0.65 OK\n","IoU 0.7 OK\n","IoU 0.75 OK\n","IoU 0.8 OK\n","IoU 0.85 OK\n","IoU 0.9 OK\n","IoU 0.95 OK\n","COCO primary challenge metric: 0.3691\n","PASCAL VOC 2012 metric: 0.6196\n","COCO strict metric: 0.3866\n","  0.50    0.55    0.60    0.65    0.70    0.75    0.80    0.85    0.90    0.95\n","[ 0.6196, 0.5988, 0.5486, 0.5141, 0.4585, 0.3866, 0.3325, 0.1901, 0.0417, 0.0 ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G1VzKUWlutNE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617147303887,"user_tz":-120,"elapsed":173932,"user":{"displayName":"Etienne Platini","photoUrl":"","userId":"01355754336263421503"}},"outputId":"9b757f6a-47fd-4490-b7ee-623a03430487"},"source":["# Compute primary challenge metric of COCO: mAP@.5:0.05:0.95\n","mAPs = []\n","\n","# range() only works with ints...\n","for IoU in range(50, 100, 5):\n","  IoU /= 100\n","  mAPs.append(compute_mAP(inference_model, box_test_set, 15, inference_config, IoU_threshold = IoU ))\n","  print(f'IoU {IoU} OK')\n","\n","print(f'COCO primary challenge metric: {round(np.mean(mAPs), 4)}')\n","print(f'PASCAL VOC 2012 metric: {round(mAPs[0], 4)}')\n","print(f'COCO strict metric: {round(mAPs[5], 4)}')\n","print(f'  0.50    0.55    0.60    0.65    0.70    0.75    0.80    0.85    0.90    0.95')\n","print(f'[ {round(mAPs[0], 4)}, {round(mAPs[1], 4)}, {round(mAPs[2], 4)}, {round(mAPs[3], 4)}, {round(mAPs[4], 4)}, {round(mAPs[5], 4)}, {round(mAPs[6], 4)}, {round(mAPs[7], 4)}, {round(mAPs[8], 4)}, {round(mAPs[9], 4)} ]')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["IoU 0.5 OK\n","IoU 0.55 OK\n","IoU 0.6 OK\n","IoU 0.65 OK\n","IoU 0.7 OK\n","IoU 0.75 OK\n","IoU 0.8 OK\n","IoU 0.85 OK\n","IoU 0.9 OK\n","IoU 0.95 OK\n","COCO primary challenge metric: 0.8189\n","PASCAL VOC 2012 metric: 0.9889\n","COCO strict metric: 0.9667\n","  0.50    0.55    0.60    0.65    0.70    0.75    0.80    0.85    0.90    0.95\n","[ 0.9889, 0.9889, 0.9889, 0.9889, 0.9667, 0.9667, 0.9, 0.9, 0.4833, 0.0167 ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J1vXsTf_3e6H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617145978826,"user_tz":-120,"elapsed":26129,"user":{"displayName":"Etienne Platini","photoUrl":"","userId":"01355754336263421503"}},"outputId":"35cda920-5ec8-4f0f-cc13-594509684b31"},"source":["# Time the detection\n","nb_images = 100\n","images = []\n","for image_id in range(nb_images):\n","  image_raw = TV_train_set.load_image(image_id)\n","  image = mold_image(image_raw, inference_config)\n","  images.append(image)\n","  \n","print(\"Image loading done\")\n","\n","start_time = time.time()\n","for i in range(nb_images):\n","  _ = inference_model.detect([images[i]], verbose=0)\n","end_time = time.time()\n","\n","print(f'Time elapsed: {round(end_time - start_time, 2)} second(s)')\n","print(f'Mean detection time: {round((end_time - start_time)/nb_images, 2)} second')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Image loading done\n","Time elapsed: 22.96 second(s)\n","Mean detection time: 0.23 second\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WzYZqa8aVGzt","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1617146734706,"user_tz":-120,"elapsed":890,"user":{"displayName":"Etienne Platini","photoUrl":"","userId":"01355754336263421503"}},"outputId":"73409ce3-afb5-46ad-bc65-c00c1d8d61c7"},"source":["# Useful to plot loss and val loss\n","def plot_loss_vs_val_loss(loss, val_loss, name):\n","\n","  plt.plot(loss, label='loss', color='tab:blue')\n","  plt.plot(val_loss, label='val_loss', color='tab:orange')\n","\n","  plt.xlim(0, 40)\n","  plt.ylim(0, 2)\n","\n","  plt.legend()\n","  plt.xlabel('Epochs') \n","  plt.ylabel('Loss') \n","  plt.title(f'Loss vs validation loss by epoch {name}') \n","  plt.show()\n","\n","loss = [ 0.863, 0.81, 0.624, 0.571, 0.493, 0.361, 0.277, 0.275, 0.326, 0.214, 0.153, 0.164, 0.187, 0.154, 0.183, 0.136, 0.15, 0.23, 0.131, 0.114, 0.134, 0.226, ]\n","val_loss = [ 0.468, 0.732, 0.542, 1.92, 0.254, 0.156, 0.651, 0.178, 0.377, 0.271, 0.235, 0.114, 0.28, 0.145, 0.081, 0.353, 0.138, 0.105, 0.121, 0.087, 0.085, 0.199, ]\n","\n","plot_loss_vs_val_loss(loss, val_loss, 'model trained on TV dataset')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"kAPQ7DS7MGwV"},"source":["# Useful to compare the inference times\n","import matplotlib.pyplot as plt\n","\n","x = ['i7-10750H', 'GTX 850M', 'RTX 2060 6G', 'Tesla T4', 'Paper']\n","y = [ 1.34, 1.44, 0.6, 0.23, 0.2 ]\n","\n","plt.bar(x, y)\n","\n","plt.xlabel('hardware') \n","plt.ylabel('Time of inference in seconds') \n","plt.title('Time of inference by hardware') "],"execution_count":null,"outputs":[]}]}